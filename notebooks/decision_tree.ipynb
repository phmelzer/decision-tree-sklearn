{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4664d535",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4852c4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Fundamentals](#1)\n",
    "    - [1.1 - Characteristics](#1.1)\n",
    "    - [1.2 - Classification Trees](#1.2)\n",
    "        - [1.2.1 - Measuring purity](#1.2.1)\n",
    "        - [1.2.2 - Information Gain](#1.2.2)\n",
    "        - [1.2.3 - Examples](#1.2.3)        \n",
    "    - [1.3 - Regression Trees](#1.3)\n",
    "        - [1.3.1 - Calculating Variance](#1.3.1)\n",
    "        - [1.3.2- Example](#1.3.2)\n",
    "    - [1.4 - Decision Tree Learning](#1.4)\n",
    "    - [1.5 - One-hot encoding for categorical features](#1.5)\n",
    "    - [1.6 - Continuous valued features](#1.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680199c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf104080",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 - Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b11779",
   "metadata": {},
   "source": [
    "A Decision Tree is a popular recursive machine learning algorithm for classification and regression tasks.\n",
    "\n",
    "Advantages:\n",
    "- Easy to understand, interpret and visualize\n",
    "- Works with categorical and numeric values\n",
    "- Non-linear relationships between variables do not affect the accuracy of the tree\n",
    "- Feature importance can be displayed \n",
    "- Requires minimal preparation or data cleaning before use\n",
    "\n",
    "Disadvantages:\n",
    "- Highly sensitive to small changes of the data (unstable)\n",
    "- In case of unbalanced training data this so-called bias can also be present in the tree\n",
    "- Prone to overfitting (as a result, they do not generalize well to previously unseen data)\n",
    "- Can not extrapolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf05590",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "## 1.2 - Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bd797",
   "metadata": {},
   "source": [
    "<a name='1.2.1'></a>\n",
    "### 1.2.1 - Measuring purity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348fd97",
   "metadata": {},
   "source": [
    "#### Entropy \n",
    "\n",
    "The Entropy function is a measure of the impurity of a set of data. The optimum split is chosen by the features with less entropy. It gets its maximum value when the probability of the two classes is the same and a node is pure when the entropy has its minimum value, which is 0.\n",
    "\n",
    "The Entropy is calculated using the following formula: \n",
    "    \n",
    "$$H(p1) = -p_{1}log_{2}(p_{1}) - p_{0}log_{2}(p_{0}) = -p_{1}log_{2}(p_{1}-(1-p_{1})log_{2}(1-p_{1})$$\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#### Gini\n",
    "    \n",
    "The Gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled.\n",
    "The minimum value of the Gini index is 0. This happens when the node is pure, this means that all the contained elements in the node are of one unique class. Therefore, this node will not be split again. Thus, optimum split is chosen by the features with less Gini index. Moreover, it gets the maximum value when the probability of the two classes are the same.\n",
    "\n",
    "The Gini impurity is calculated using the following formular:\n",
    "    \n",
    "$$GiniIndex = 1- \\sum_{j}p_{j}^{2}$$\n",
    "    \n",
    "#### Comparing Entropy and Gini criterion\n",
    "    \n",
    "The Gini criterion is much faster because it is less computationally expensive (Entropy makes use of logarithms). On the other hand, the Entropy criterion provides slightly better results.\n",
    "\n",
    "<img src=\"images/entropy_and_gini.png\" style=\"width:200;height:200px;\">\n",
    "<caption><center><font><b>Figure 1</b>: Entropy and Gini function</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b19f6",
   "metadata": {},
   "source": [
    "<a name='1.2.2'></a>\n",
    "### 1.2.2 - Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39a3a4",
   "metadata": {},
   "source": [
    "When building a decision tree for classification, the way we'll decide what feature to split on at the node will be based on what choice of feature reduces entropy the most. Reduces entropy or reduces impurity or maximizes purity. In decision tree learning, the reduction of entropy is called information gain.    \n",
    "    \n",
    "General formular for computing information gain:\n",
    "\n",
    "$$H(p_{1}^{root}) - (w^{left} H(p_{1}^{left}) + w^{right} H(p_{1}^{right}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d613e",
   "metadata": {},
   "source": [
    "<a name='1.2.3'></a>\n",
    "### 1.2.3 - Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae695b5",
   "metadata": {},
   "source": [
    "Let's calculate the Entropy with some examples for different datasets of cats and dogs (fraction of examples that are cats defined as $p_{1}$).\n",
    "    \n",
    "<img src=\"images/entropy_examples.jpg\" style=\"width:200;height:200px;\">\n",
    "<caption><center><font><b>Figure 2</b>: Entropy examples</center></caption>\n",
    "    \n",
    "Now we're calculating the information gain.\n",
    "    \n",
    "<img src=\"images/information_gain.jpg\" style=\"width:300;height:300px;\">\n",
    "<caption><center><font><b>Figure 3</b>: Information Gain Example 1</center></caption>\n",
    "    \n",
    "    \n",
    "<img src=\"images/information_gain_example.jpg\" style=\"width:300;height:300px;\">\n",
    "<caption><center><font><b>Figure 4</b>: Information Gain Example 2</center></caption>\n",
    "    \n",
    "Note: \"$0 log(0)$\" = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6abf1",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "## 1.3 - Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17891e",
   "metadata": {},
   "source": [
    "<a name='1.3.1'></a>\n",
    "### 1.3.1 - Calculating variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3bfcb",
   "metadata": {},
   "source": [
    "For regression trees, rather than trying to reduce entropy, we instead try to reduce the variance.\n",
    "\n",
    "The variance is calculated using the following formula:\n",
    "\n",
    "$$\\sigma^{2} = \\frac{\\sum(X - \\mu)^{2}}{N} $$\n",
    "\n",
    "1. Calculate the mean\n",
    "2. Calculate each deviation from the mean\n",
    "3. Square each deviation from the mean\n",
    "4. Sum up all squares\n",
    "5. Devide the sum of squares by N\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c610d",
   "metadata": {},
   "source": [
    "<a name='1.3.2'></a>\n",
    "### 1.3.2 - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185d3a9",
   "metadata": {},
   "source": [
    "<img src=\"images/regression_tree_example.jpg\" style=\"width:400;height:400px;\">\n",
    "<caption><center><font><b>Figure 5</b>: Regression tree example</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa62e2",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "## 1.4 - Decision Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9272b",
   "metadata": {},
   "source": [
    "1. Start with all examples at the root node\n",
    "2. Calculate information gain or variance for all possible features, and pick the one with the highest information gain or highest variance reduction\n",
    "3. Split dataset according to selected feature, and create left and right branches of the tree\n",
    "4. Keep repeating splitting process until stopping criteria is met:\n",
    " - When a node is 100% a single class\n",
    " - When splitting a node will results in the tree exceeding a minimum depth\n",
    " - Information gain from additional splits is less than a threshold\n",
    " - When number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71705364",
   "metadata": {},
   "source": [
    "<a name='1.5'></a>\n",
    "## 1.5 - One-hot encoding for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa01e8",
   "metadata": {},
   "source": [
    "If a categorical feature can take on *k* values, create *k* binary features (0 or 1 valued). One-hot encoding is not limited to decision trees, but also used for other algorithms (e.g. neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38caf364",
   "metadata": {},
   "source": [
    "<a name='1.6'></a>\n",
    "## 1.6 - Continuous valued features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0e468",
   "metadata": {},
   "source": [
    "When splitting on continuous valued features we have to consider many different values for the threshold and then pick the one that is the best, which means the one that results in the best information gain. In general one way to choose the values considered for a threshold is to sort all of the examples according to the value of this feature and take all the values that are mid points between the sorted list of training examples.\n",
    "\n",
    "\n",
    "And what we should do when constraints splitting one the weight feature is to consider many different values of this threshold and then to pick the one that is the best. And by the best I mean the one that results in the best information gain.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/splitting_on_continuous_variables.jpg\" style=\"width:400;height:400px;\">\n",
    "<caption><center><font><b>Figure 5</b>: Splitting on continuous variables example</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b49712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce113f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
