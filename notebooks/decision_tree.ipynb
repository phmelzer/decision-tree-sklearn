{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4664d535",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4852c4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Fundamentals](#1)\n",
    "    - [1.1 - Characteristics](#1.1)\n",
    "    - [1.2 - Measuring purity](#1.2)\n",
    "    - [1.3 - Information Gain](#1.3)\n",
    "    - [1.4 - Decision Tree Learning](#1.4)\n",
    "    - [1.5 - One-hot encoding for categorical features](#1.5)\n",
    "    - [1.6 - Continuous valued features](#1.6)\n",
    "    - [1.7 - Regression Trees](#1.7)\n",
    "- [2 - Example](#2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680199c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf104080",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 - Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b11779",
   "metadata": {},
   "source": [
    "A Decision Tree is a popular recursive machine learning algorithm for classification and regression tasks.\n",
    "\n",
    "Advantages:\n",
    "- Easy to understand, interpret and visualize\n",
    "- Works with categorical and numeric values\n",
    "- Non-linear relationships between variables do not affect the accuracy of the tree\n",
    "- Feature importance can be displayed \n",
    "- Requires minimal preparation or data cleaning before use\n",
    "\n",
    "Disadvantages:\n",
    "- Highly sensitive to small changes of the data (unstable)\n",
    "- In case of unbalanced training data this so-called bias can also be present in the tree\n",
    "- Prone to overfitting (as a result, they do not generalize well to previously unseen data)\n",
    "- Can not extrapolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bd797",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "## 1.2 - Measuring purity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348fd97",
   "metadata": {},
   "source": [
    "### Entropy function\n",
    "\n",
    "The Entropy function is a measure of the impurity of a set of data (there are also other functions like the gini function). Let's go through some examples.\n",
    "\n",
    "Let's define the fraction of examples that are cats as $p_{1}$.\n",
    "\n",
    "$p_{1}$ = fraction of examples that are cats.\n",
    "\n",
    "$p_{0} = 1 - p_{1}$\n",
    "\n",
    "The Entropy function looks like this:\n",
    "<img src=\"images/entropy_function.jpg\" style=\"width:200;height:200px;\">\n",
    "<caption><center><font><b>Figure 2</b>: Entropy function</center></caption>\n",
    "    \n",
    "and the general equation to calculate the entropy is this: \n",
    "    \n",
    "$$H(p1) = -p_{1}log_{2}(p_{1}) - p_{0}log_{2}(p_{0}) = -p_{1}log_{2}(p_{1}-(1-p_{1})log_{2}(1-p_{1})$$\n",
    "    \n",
    "\n",
    "Note: \"$0 log(0)$\" = 0\n",
    "    \n",
    "### Gini function\n",
    "\n",
    "    \n",
    "xyz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b19f6",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "## 1.3 - Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39a3a4",
   "metadata": {},
   "source": [
    "When building a decision tree, the way we'll decide what feature to split on at the node will be based on what choice of feature reduces entropy the most. Reduces entropy or reduces impurity or maximizes purity. In decision tree learning, the reduction of entropy is called information gain.\n",
    "\n",
    "<img src=\"images/information_gain.jpg\" style=\"width:200;height:200px;\">\n",
    "<caption><center><font><b>Figure 2</b>: Information Gain</center></caption>\n",
    "    \n",
    "    \n",
    "General formular for computing information gain: $H(p_{1}^{root}) - (w^{left} H(p_{1}^{left}) + w^{right} H(p_{1}^{right}))$\n",
    "    \n",
    "<img src=\"images/information_gain_example.jpg\" style=\"width:200;height:200px;\">\n",
    "<caption><center><font><b>Figure 2</b>: Information Gain Example</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa62e2",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "## 1.4 - Decision Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9272b",
   "metadata": {},
   "source": [
    "1. Start with all examples at the root node\n",
    "2. Calculate information gain for all possible features, and pick the one with the highest information gain\n",
    "3. Split dataset according to selected feature, and create left and right branches of the tree\n",
    "4. Keep repeating splitting process until stopping criteria is met:\n",
    " - When a node is 100% a single class\n",
    " - When splitting a node will results in the tree exceeding a minimum depth\n",
    " - Information gain from additional splits is less than a threshold\n",
    " - When number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16852f-0ddd-41bf-bd51-82099a1bfa63",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2 - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6831103",
   "metadata": {},
   "source": [
    "Let's go through some examples for different datasets of cats and dogs:\n",
    "    \n",
    "<img src=\"images/entropy_examples.jpg\" style=\"width:200;height:200px;\">\n",
    "<caption><center><font><b>Figure 2</b>: Entropy examples</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42e37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
